{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import heapq\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def lsaSummarize(text):\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Create a list of stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    # Create a list of cleaned sentences\n",
    "    clean_sentences = [sent.lower() for sent in sentences]\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "    tfidf_matrix = vectorizer.fit_transform(clean_sentences)\n",
    "\n",
    "    svd = TruncatedSVD(n_components=5, random_state=0)\n",
    "    svd_matrix = svd.fit_transform(tfidf_matrix)\n",
    "\n",
    "    # Calculate the sentence scores based on the SVD matrix\n",
    "    scores = np.sum(svd_matrix, axis=1)\n",
    "\n",
    "    # Sort the sentences by score and select the top 3\n",
    "    ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "    summary = ' '.join([ranked_sentences[i][1] for i in range(3)])\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarizer(text):\n",
    "    article_text = text.replace(\"\\n\", \" \")\n",
    "    article_text = re.sub(r\"\\[[0-9]*\\]\", \" \", article_text)\n",
    "    article_text = re.sub(r\"\\s+\", \" \", article_text)\n",
    "\n",
    "    # Remove special characters and digits\n",
    "    formatted_article_text = re.sub(\"[^a-zA-Z]\", \" \", article_text)\n",
    "    formatted_article_text = re.sub(r\"\\s+\", \" \", formatted_article_text)\n",
    "\n",
    "    sentence_list = nltk.sent_tokenize(article_text)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "    word_frequencies = {}\n",
    "    for word in nltk.word_tokenize(formatted_article_text):\n",
    "        if word not in stopwords:\n",
    "            if word not in word_frequencies.keys():\n",
    "                word_frequencies[word] = 1\n",
    "            else:\n",
    "                word_frequencies[word] += 1\n",
    "    maximum_frequency = max(word_frequencies.values())\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word] = word_frequencies[word] / maximum_frequency\n",
    "\n",
    "    sentence_scores = {}\n",
    "    for sent in sentence_list:\n",
    "        for word in nltk.word_tokenize(sent.lower()):\n",
    "            if word in word_frequencies.keys():\n",
    "                if len(sent.split(\" \")) < 30:\n",
    "                    if sent not in sentence_scores.keys():\n",
    "                        sentence_scores[sent] = word_frequencies[word]\n",
    "                    else:\n",
    "                        sentence_scores[sent] += word_frequencies[word]\n",
    "\n",
    "    summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\n",
    "    summary = \" \".join(summary_sentences)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarizer_3(text):\n",
    "    from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "    # Load the tokenizer and model\n",
    "    tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "    model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "\n",
    "    # Encode the input text\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "\n",
    "    # Generate the summary\n",
    "    summary_ids = model.generate(input_ids, max_length=50, num_beams=4, length_penalty=2.0, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Print the summary\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_summaries(text):\n",
    "    summary_1 = lsaSummarize(text)\n",
    "    summary_2 = summarizer(text)\n",
    "    summary_3 = summarizer_3(text)\n",
    "    summaries = [summary_1, summary_2, summary_3]\n",
    "    scores = {}\n",
    "    for i in range(3):\n",
    "        for j in range(i+1,3):\n",
    "            # Tokenize the summaries\n",
    "            summary1_tokens = word_tokenize(summaries[i])\n",
    "            summary2_tokens = word_tokenize(summaries[j])\n",
    "\n",
    "            # Calculate the Jaccard similarity score\n",
    "            intersection = set(summary1_tokens).intersection(summary2_tokens)\n",
    "            union = set(summary1_tokens).union(summary2_tokens)\n",
    "            jaccard_score = len(intersection) / len(union)\n",
    "\n",
    "            # Add the score to the dictionary\n",
    "            scores[(i,j)] = jaccard_score\n",
    "\n",
    "    # Find the maximum score\n",
    "    max_score = max(scores.values())\n",
    "\n",
    "    # Find the summaries with the maximum score\n",
    "    best_summaries = []\n",
    "    for key, value in scores.items():\n",
    "        if value == max_score:\n",
    "            best_summaries.append(key[0])\n",
    "            best_summaries.append(key[1])\n",
    "\n",
    "# Return the best summary\n",
    "    return summaries[best_summaries[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\priyanshu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A gentle breeze blew through the trees, rustling the leaves and carrying the sweet scent of flowers. The sun had set over the horizon, casting a warm orange glow across the sky. The ground was covered in a soft layer of dew, making it glisten in the moonlight.\n"
     ]
    }
   ],
   "source": [
    "text = \"The sun had set over the horizon, casting a warm orange glow across the sky. As the night fell, the stars slowly emerged, twinkling like diamonds in the sky. A gentle breeze blew through the trees, rustling the leaves and carrying the sweet scent of flowers. In the distance, a lone wolf howled, adding to the eerie beauty of the night. A stream flowed nearby, its gentle gurgling filling the air with a soothing melody. The ground was covered in a soft layer of dew, making it glisten in the moonlight. A fire crackled nearby, providing warmth and comfort to those gathered around it. It was a peaceful night, a night to be cherished and remembered for years to come.\"\n",
    "\n",
    "# Call the function and print the result\n",
    "summarize = compare_summaries(text)\n",
    "print(summarize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def translate(summary):    \n",
    "    translated_text = \"\"\n",
    "    length = len(summary)\n",
    "    parts = [summary[i:i+500] for i in range(0, length, 500)]\n",
    "    for part in parts:\n",
    "        url = \"https://api.mymemory.translated.net/get\"\n",
    "        params = {\n",
    "            \"q\": part,\n",
    "            \"langpair\": \"en|ne\",\n",
    "        }\n",
    "        response = requests.get(url, params=params)\n",
    "        data = response.json()\n",
    "        translated_text += data[\"responseData\"][\"translatedText\"]\n",
    "\n",
    "    return translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\priyanshu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A gentle breeze blew through the trees, rustling the leaves and carrying the sweet scent of flowers. The sun had set over the horizon, casting a warm orange glow across the sky. The ground was covered in a soft layer of dew, making it glisten in the moonlight.\n"
     ]
    }
   ],
   "source": [
    "text = \"The sun had set over the horizon, casting a warm orange glow across the sky. As the night fell, the stars slowly emerged, twinkling like diamonds in the sky. A gentle breeze blew through the trees, rustling the leaves and carrying the sweet scent of flowers. In the distance, a lone wolf howled, adding to the eerie beauty of the night. A stream flowed nearby, its gentle gurgling filling the air with a soothing melody. The ground was covered in a soft layer of dew, making it glisten in the moonlight. A fire crackled nearby, providing warmth and comfort to those gathered around it. It was a peaceful night, a night to be cherished and remembered for years to come.\"\n",
    "\n",
    "# Call the function and print the result\n",
    "summarize = compare_summaries(text)\n",
    "print(summarize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "मन्द हावा रूखहरूबाट बगिरहेको थियो, पातहरू र फूलहरूको मीठो सुगन्ध बोकेको थियो। सूर्य क्षितिजमा अस्ताएको थियो, आकाशमा न्यानो सुन्तलाको चमक फ्याँकिएको थियो। जमिन शीतको नरम तहले ढाकिएको थियो, चन्द्रमामा चम्किरहेको थियो।\n"
     ]
    }
   ],
   "source": [
    "translated = translate(summarize)\n",
    "print(translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
